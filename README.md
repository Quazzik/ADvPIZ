# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #1 выполнил(а):
- Коротков Юрий Артемович
- НМТ-213901
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.

## Цель работы
Детальнее изучить работу ML-Агента, контролирующего экономику в абстрактной системе созданной в Unity

## Задание 1
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели.

О влиянии некоторых параметров на обучение я догадывался по названию смежных переменных в блоке (hyperparameters), поэтому решил изменить все разом чтобы увидеть хоть и не предсказуемое, но ожидаемое изменение результатов. Измененный конфиг представлен ниже:

```C#

behaviors:
  Economic:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 3.0e-4
      learning_rate_schedule: linear
      beta: 1.0e-2
      epsilon: 0.3
      lambd: 0.92
      num_epoch: 7      
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    checkpoint_interval: 500000
    max_steps: 750000
    time_horizon: 64
    summary_freq: 5000
    self_play:
      save_steps: 20000
      team_change: 100000
      swap_steps: 10000
      play_against_latest_model_ratio: 0.5
      window: 10
      
```

Используя эту конфигурацию я провел обучение. Понимая что ничего хорошего ждать от нее не стоит я остановил выполнение после 40.000 тиков. Результат представлен ниже:

![image](https://user-images.githubusercontent.com/113617617/205116717-29ce3dee-6ebf-4e17-a8f2-5ecf2c28f146.png)

И если эти графики показали хоть и нестабильный, но адекватный результат, то Policy, в часности энтропия выглядели не очень хорошо:

![image](https://user-images.githubusercontent.com/113617617/205117429-56c0ecf4-777a-46ea-9618-eefa973c1d7a.png)

По этим графикам видно что конфигурация мягко говоря неэффективная.

В ходе нескольких экспериментов я понял что мне не хватит терпения разобраться во всем опытным путем и через пару страниц документации я нашел функциональное значение тех самых параметров которые хаотично изменял:

``` beta ``` - Сила регуляризации энтропии, которая делает политику «более случайной». Это гарантирует, что агенты должным образом исследуют пространство действия во время обучения. Увеличение этого параметра обеспечит выполнение большего количества случайных действий. Это должно быть скорректировано таким образом, чтобы энтропия (измеряемая с помощью TensorBoard) медленно уменьшалась вместе с увеличением вознаграждения. Если энтропия падает слишком быстро, увеличьте бета. Если энтропия падает слишком медленно, уменьшите beta.

``` epsilon ``` - Влияет на скорость изменения политики во время обучения. Соответствует допустимому порогу расхождения между старой и новой политикой при обновлении градиентного спуска. Установка небольшого значения этого параметра приведет к более стабильным обновлениям, но также замедлит процесс обучения.

``` lambd ``` - 	Параметр регуляризации лямбда, используемый при расчете обобщенной оценки преимущества. Это можно рассматривать как то, насколько агент полагается на свою текущую оценку стоимости при вычислении обновленной оценки стоимости. Низкие значения соответствуют большему полаганию на текущую оценку ценности (что может быть высоким смещением), а высокие значения соответствуют большему полаганию на фактические вознаграждения, полученные в среде (что может быть высокой дисперсией). Параметр обеспечивает компромисс между ними, и правильное значение может привести к более стабильному процессу обучения.

``` num_epoch ``` - Количество проходов через буфер опыта при выполнении оптимизации градиентного спуска. Чем больше размер партии, тем больше это допустимо. Уменьшение этого параметра обеспечит более стабильные обновления за счет более медленного обучения.

И основываясь на полученных знаниях я собрал новый конфиг. С ним результат будет характеризоваться предстаказуемостью и стабильным ростом:

``` C#

behaviors:
  Economic:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 3.0e-4
      learning_rate_schedule: linear
      beta: 1.0e-3
      epsilon: 0.1
      lambd: 0.94
      num_epoch: 5      
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    checkpoint_interval: 500000
    max_steps: 50000
    time_horizon: 64
    summary_freq: 5000
    self_play:
      save_steps: 50000
      team_change: 100000
      swap_steps: 10000
      play_against_latest_model_ratio: 0.5
      window: 10

```

После подключения нового конфига график наград стал более предсказуем и начал условно стабильно расти, графики которые должны были падать стали падать, а энтропия к концу обучения начала снижаться т.к. ML-агент начал дорабатывать приближенные к идеальному параметры. Сами графики:

![image](https://user-images.githubusercontent.com/113617617/205107863-60269008-5ece-4816-a57b-f9bdec4e4a92.png)

![image](https://user-images.githubusercontent.com/113617617/205108439-bd39d6fe-d14c-49b6-968e-3cb7f54c90dc.png)


## Задание 2, 3
### Описание результатов выведенных в TensorBoard

![image](https://user-images.githubusercontent.com/113617617/205109158-b3fb737b-90bf-4bb6-bae8-7014762219f7.png)

На графиках изображены вознаграждения с 4 конфигурациями:

``` Синий ``` - стандартный вариант, идеальный. Совершались действия в сумме дающие постоянный прирост, поэтому она "зависла под потолком"

``` Красный ``` - мой первый конфиг, случайный, дающий случаные значение, поэтому и получающий нестабильную награду

``` Голубой ``` - мой измененный конфиг, постоянно дорабатывающийся и получающий стабильно больше награды с каждой итерацией

``` Оранжевый ``` - тест первой версии измененного конфига, начал получать меньше и за это был модифицирован до показанной версии

## Выводы

В ходе лабораторной работы я полностью разобрался в конфиге для ML-агента, понял какие параметры за что отвечают и подобрал их так, чтобы ML-агент  обучался эффективно. Так же пришлось ознакомиться с документацией к MLAgents т.к. самостоятельные попытки подобрать значения не увенчались успехом. Так же я ознакомился с tensorflow - инструментом для графического отображения поведения нейросети во время обучения.  В итоге я закрепил навыки полученные в лабораторной работе №3, где впервые встретился с ML-агентом и научился работать с tensorflow.

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
